{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import sklearn_crfsuite\n",
    "from sklearn.model_selection import LeaveOneOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定義\n",
    "WINDOW_SIZE = 2\n",
    "result_path = \"./data/output/Chapter6\"\n",
    "\n",
    "# 記事idの読み込み\n",
    "df = pd.read_csv(\"./data_id.csv\",header=None)\n",
    "data_id_list = df.T.values.tolist()[0]\n",
    "\n",
    "corpus_file = 'corpus_5w1hs_ch6.txt' # コーパスの読み込み\n",
    "\n",
    "# ファイル書き出し用\n",
    "ex_file = \"{0}/r3_extraction_result.csv\".format(result_path) # 全体の結果タイプ数\n",
    "f_file = \"{0}/r3_failure_result.csv\".format(result_path) # 失敗タイプのデータ\n",
    "ex_txt = \"{0}/r3_failure_result.txt\".format(result_path) # 詳細結果(テキスト)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# コーパス読み込み\n",
    "import codecs\n",
    "class CorpusReader(object):\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        with codecs.open(path, encoding='utf-8') as f:\n",
    "            sent = []\n",
    "            sents = []\n",
    "            for line in f:\n",
    "                if line == '\\n':\n",
    "                    sents.append(sent)\n",
    "                    sent = []\n",
    "                    continue\n",
    "                morph_info = line.strip().split('\\t')\n",
    "                sent.append(morph_info) # 形態素の保存 \n",
    "        train_num = int(len(sents) * 0.9) # 9割を学習に、1割をテストに\n",
    "        self.__train_sents = sents[:train_num]\n",
    "        self.__test_sents = sents[train_num:]\n",
    "        self.__all_sents = sents\n",
    "        \n",
    "    def iob_sents(self, name):\n",
    "        if name == 'train':\n",
    "            return self.__train_sents\n",
    "        elif name == 'test':\n",
    "            return self.__test_sents\n",
    "        elif name == 'all':\n",
    "            return self.__all_sents\n",
    "        else:\n",
    "            return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 文字種取得\n",
    "def is_hiragana(ch):\n",
    "    return 0x3040 <= ord(ch) <= 0x309F \n",
    "    # ひらがな：True or False\n",
    "\n",
    "def is_katakana(ch):\n",
    "    return 0x30A0 <= ord(ch) <= 0x30FF\n",
    "    # カタカタ：True or False\n",
    "\n",
    "def get_character_type(ch): # 文字種を取得する\n",
    "    if ch.isspace(): # 空白の場合\n",
    "        return 'ZSPACE'\n",
    "    elif ch.isdigit(): # 数字の場合\n",
    "        return 'ZDIGIT'\n",
    "    elif ch.islower(): # 小文字の場合\n",
    "        return 'ZLLET'\n",
    "    elif ch.isupper(): # 大文字の場合\n",
    "        return 'ZULET'\n",
    "    elif is_hiragana(ch): # ひらがなの場合\n",
    "        return 'HIRAG'\n",
    "    elif is_katakana(ch): # カタカナの場合\n",
    "        return 'KATAK'\n",
    "    else: # それ以外\n",
    "        return 'OTHER'\n",
    "\n",
    "def get_character_types(string): # 文字列の文字種を変換する\n",
    "    character_types = map(get_character_type, string)\n",
    "    character_types_str = '-'.join(sorted(set(character_types)))\n",
    "\n",
    "    return character_types_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 品詞細分類の取得\n",
    "def extract_pos_with_subtype(morph):\n",
    "    return '-'.join(morph[1:3]) # 品詞分類(品詞大分類-小分類)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 単語を特徴量に変換する\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    chtype = get_character_types(sent[i][0]) # 文字種取得    \n",
    "    postag = extract_pos_with_subtype(sent[i]) # 品詞分類(品詞大分類-小分類)取得\n",
    "    bnstlen = sent[i][3] # 固有表現\n",
    "    \n",
    "    # 該当単語の前後2文字の単語の特徴を用意\n",
    "    features = [ \n",
    "        'bias',\n",
    "        'word=' + word,\n",
    "        'type=' + chtype,\n",
    "        'postag=' + postag,\n",
    "        'bnstlen=' + bnstlen,\n",
    "    ]\n",
    "    \n",
    "    window_ids = []\n",
    "    window_ids.extend([i for i in range(WINDOW_SIZE*(-1),0)])\n",
    "    window_ids.extend([i+1 for i in range(WINDOW_SIZE)])\n",
    "    \n",
    "    for window_id in window_ids:\n",
    "    \n",
    "        if window_id < 0: # 該当単語の前をとる\n",
    "            \n",
    "            if i >= (window_id*-1) : \n",
    "                word = sent[i+window_id][0]\n",
    "                chtype = get_character_types(sent[i+window_id][0])\n",
    "                postag = extract_pos_with_subtype(sent[i+window_id])\n",
    "                bnstlen = sent[i+window_id][3]\n",
    "                features.extend([\n",
    "                    str(window_id) + ':word=' + word,\n",
    "                    str(window_id) + ':type=' + chtype,\n",
    "                    str(window_id) + ':postag=' + postag,\n",
    "                    str(window_id) + ':bnstlen=' + bnstlen,\n",
    "                ])\n",
    "            else: # それ以外は、BOS\n",
    "                features.append('BOS')\n",
    "                \n",
    "        else: # 該当単語の後ろをとる\n",
    "            if i < len(sent)-window_id:\n",
    "                word = sent[i+window_id][0]\n",
    "                chtype = get_character_types(sent[i+window_id][0])\n",
    "                postag = extract_pos_with_subtype(sent[i+window_id])\n",
    "                bnstlen = sent[i+window_id][3]\n",
    "                features.extend([\n",
    "                    \"+\" + str(window_id) + ':word=' + word,\n",
    "                    \"+\" + str(window_id) + ':type=' + chtype,\n",
    "                    \"+\" + str(window_id) + ':postag=' + postag,\n",
    "                    \"+\" + str(window_id) + ':bnstlen=' + bnstlen,\n",
    "                ])\n",
    "            else: # それ以外は、EOS\n",
    "                features.append('EOS')                        \n",
    "            \n",
    "    return features    \n",
    "\n",
    "def sent2features(sent): # 情報系列から特徴を取得\n",
    "    # 単語ごとに特徴変換していく\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent): # 情報系列からラベル[B、I、O]を取得\n",
    "    return [morph[-1] for morph in sent]\n",
    "\n",
    "def sent2tokens(sent): # 情報系列から単語原文を取得\n",
    "    return [morph[0] for morph in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ラベルごとに単語を保存する\n",
    "def label_report(text,label_data):\n",
    "    wh_dic = {\"WHERE\":[], \"WHEN\":[], \"WHO\":[], \"WHAT\":[], \"HOW\":[], \"WHY\":[],\"SERIF\":[],\"O\":[]}\n",
    "    \n",
    "    set_label = \"\"\n",
    "    result_word = \"\"\n",
    "    wc = -1\n",
    "    wc_l = []\n",
    "    \n",
    "    for i,word in enumerate(text):\n",
    "\n",
    "        ld = label_data[i]\n",
    "        \n",
    "        # B,Iタグをとりあえず、無視する\n",
    "        if \"B-\" in ld:\n",
    "            ld = ld.replace(\"B-\", \"\")\n",
    "        elif \"I-\" in ld:\n",
    "            ld = ld.replace(\"I-\", \"\")\n",
    "            \n",
    "        # 最後の要素のとき\n",
    "        if i == (len(text)-1):\n",
    "            wh_dic[set_label].append([result_word,wc_l])\n",
    "            break\n",
    "        \n",
    "        # ラベルごとに辞書へ保存する。\n",
    "        if set_label != \"\" and set_label != ld: \n",
    "            # 辞書に保存            \n",
    "            wh_dic[set_label].append([result_word,wc_l])\n",
    "            \n",
    "            # 初期化\n",
    "            result_word = \"\"\n",
    "            wc_l = []\n",
    "\n",
    "        # 同一ラベルの情報を結合\n",
    "        for wi in range(len(word)):\n",
    "            wc += 1\n",
    "            wc_l.append(wc)\n",
    "        \n",
    "        # 同一ラベルの情報を結合\n",
    "        set_label = ld\n",
    "        result_word += word   \n",
    "            \n",
    "    return wh_dic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "評価用\n",
    "入力：正解データ辞書、予測データ辞書\n",
    "出力： 5W1Hごとの失敗パターン辞書\n",
    "    - 完全： 正解チャンク\n",
    "    - 一部一致： 正解チャンク, 予測チャンク\n",
    "    - ラベル誤り：正解チャンク, 予測チャンク, 正解ラベル, 予測ラベル\n",
    "    - 抽出漏れ： 正解チャンク\n",
    "    - 過度の抽出： 予測チャンク\n",
    "\"\"\"\n",
    "def eval_report(true_wh_dic,pred_wh_dic):\n",
    "    report_data = {}\n",
    "    \n",
    "    # 予測ラベルのマッチングチェックリスト (マッチしたら1)\n",
    "    check_pred_dic = {}\n",
    "    for k,v in pred_wh_dic.items():\n",
    "        if k == \"O\": # ラベルOのときの数え上げない\n",
    "            continue\n",
    "        check_pred = [0 for i in range(len(v))]\n",
    "        check_pred_dic[k] = check_pred\n",
    "        \n",
    "    # 正解ラベルのマッチングリスト\n",
    "    check_true_dic = {}\n",
    "    for k,v in true_wh_dic.items():\n",
    "        if k == \"O\": # ラベルOのときの数え上げない\n",
    "            continue\n",
    "        check_true = [0 for i in range(len(v))]\n",
    "        check_true_dic[k] = check_true\n",
    "        \n",
    "        \n",
    "        \n",
    "    for k,v in true_wh_dic.items():\n",
    "        \n",
    "        if k == \"O\": # ラベルOのときの数え上げない\n",
    "            continue\n",
    "            \n",
    "        report_v = {}\n",
    "        result_type1 = []\n",
    "        result_type2 = []\n",
    "        result_type3 = []\n",
    "        \n",
    "        for t_num, true_text in enumerate(v):\n",
    "            true_id = true_text[1] \n",
    "                \n",
    "            for anoter_pk,anoter_pts in pred_wh_dic.items(): # すべての予想ラベルから検索\n",
    "                 for p_num, anoter_pt in enumerate(anoter_pts):\n",
    "                        \n",
    "                        if anoter_pk == \"O\": # ラベルOのときの数え上げない\n",
    "                            continue\n",
    "                        \n",
    "                        pred_id = anoter_pt[1]\n",
    "                        match_n = list(set(true_id) & set(pred_id))  \n",
    "                        \n",
    "                        if k == anoter_pk and len(match_n) > 0: # 予想も正解もチャンクとラベルが同じ\n",
    "                            if len(true_id) == len(pred_id):\n",
    "                                check_pred_dic[k][p_num] = 1 \n",
    "                                check_true_dic[k][t_num] = 1\n",
    "                                result_type1.append(true_text[0]) # 完全\n",
    "                            else:\n",
    "                                check_pred_dic[k][p_num] = 1 \n",
    "                                check_true_dic[k][t_num] = 1\n",
    "                                result_type2.append([true_text[0],anoter_pt[0]]) #一部一致\n",
    "                                \n",
    "                        if k != anoter_pk and len(match_n) > 0: # 予想も正解もチャンクは同じだが、ラベルが違う\n",
    "                            result_type3.append([true_text[0],anoter_pt[0],k,anoter_pk]) # ラベル誤り\n",
    "                            check_pred_dic[anoter_pk][p_num] = 1 \n",
    "                            check_true_dic[k][t_num] = 1\n",
    "                        \n",
    "        report_v[\"完全\"] = result_type1\n",
    "        report_v[\"一部一致\"] = result_type2\n",
    "        report_v[\"ラベル誤り\"] = result_type3\n",
    "        report_v[\"抽出漏れ\"] = []\n",
    "        report_v[\"過度の抽出\"] = []\n",
    "        report_data[k] = report_v\n",
    "    \n",
    "    # 抽出漏れの処理\n",
    "    for k,v in check_true_dic.items():\n",
    "        noexits_ture = [i for i,c in enumerate(v) if c == 0]        \n",
    "        if len(noexits_ture) > 0:\n",
    "            for i in noexits_ture:\n",
    "                report_data[k][\"抽出漏れ\"].append(true_wh_dic[k][i][0]) # 抽出漏れ\n",
    "                \n",
    "    \n",
    "    # 過度の抽出の処理\n",
    "    for k,v in check_pred_dic.items():\n",
    "        miss_pred = [i for i,c in enumerate(v) if c == 0]        \n",
    "        if len(miss_pred) > 0:\n",
    "            for i in miss_pred:\n",
    "                report_data[k][\"過度の抽出\"].append(pred_wh_dic[k][i][0]) # 過度の抽出\n",
    "\n",
    "    return report_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4パターンの数を数える\n",
    "def cal_report(file_id,result,wh_label):\n",
    "    wh_num = [0 for i in range(6)]\n",
    "    wh_num[0] = file_id\n",
    "    for wh in wh_label:\n",
    "        for k,v in result[wh].items():\n",
    "            if k == \"完全\":\n",
    "                wh_num[1] += len(v)\n",
    "            elif k == \"一部一致\":\n",
    "                wh_num[2] += len(v)\n",
    "            elif k == \"ラベル誤り\":\n",
    "                wh_num[3] += len(v)\n",
    "            elif k == \"抽出漏れ\":\n",
    "                wh_num[4] += len(v)\n",
    "            elif k == \"過度の抽出\":\n",
    "                wh_num[5] += len(v)\n",
    "                    \n",
    "    return wh_num  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4タイプの結果の数を集計する\n",
    "入力：ファイルid、4タイプの結果、ラベル\n",
    "出力：1つのファイルにおける、ラベルごとのタイプごとの結果数\n",
    "\"\"\"\n",
    "def cal_report_wh(file_id,result):\n",
    "    report_data= {}\n",
    "    for label,wh_value in result.items():\n",
    "        \n",
    "        if label == \"O\": # ラベルOは除外する\n",
    "            continue\n",
    "            \n",
    "        wh_num = [0 for i in range(6)]\n",
    "        wh_num[0] = file_id\n",
    "        for rtype,value in wh_value.items():\n",
    "            if rtype == \"完全\":\n",
    "                wh_num[1] += len(value)\n",
    "            elif rtype == \"一部一致\":\n",
    "                wh_num[2] += len(value)\n",
    "            elif rtype == \"ラベル誤り\":\n",
    "                wh_num[3] += len(value)  \n",
    "            elif rtype == \"抽出漏れ\":\n",
    "                wh_num[4] += len(value)\n",
    "            elif rtype == \"過度の抽出\":\n",
    "                wh_num[5] += len(value)\n",
    "            \n",
    "        report_data[label] = wh_num\n",
    "        \n",
    "    return report_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# テンプレートを作成する\n",
    "def w2template(pred_wh_dic,text_list):\n",
    "    \n",
    "    template_txt = \"\"\n",
    "    label_ids = []\n",
    "    label_name = {}\n",
    "    \n",
    "    # 4W情報の保存\n",
    "    for k,v in pred_wh_dic.items():\n",
    "        if k == \"O\":\n",
    "            continue\n",
    "            \n",
    "        if len(v) != 0:\n",
    "            for v_i in v:\n",
    "                # 要素の最初と最後\n",
    "                sp_s = v_i[1][0]\n",
    "                sp_e = v_i[1][-1] + 1\n",
    "                \n",
    "                # ラベル情報と要素\n",
    "                label_name[sp_e] = k # ラベル情報保存\n",
    "                label_ids.append(sp_s) # 要素の最初\n",
    "                label_ids.append(sp_e) # 要素の最後\n",
    "                      \n",
    "    # テンプレート生成    \n",
    "    data_text = text_list # 1つめのデータを利用\n",
    "\n",
    "    for i,sp in enumerate(sorted(label_ids)):\n",
    "        \n",
    "        if i == 0: # 最初\n",
    "            if sp != 0:\n",
    "                template_txt += data_text[0:sp]\n",
    "                \n",
    "        elif i == (len(label_ids)-1): # 最後\n",
    "            if sp < len(data_text):\n",
    "                template_txt += \"<{0}>\".format(label_name[sp])\n",
    "                template_txt += data_text[sp:len(data_text)]\n",
    "                \n",
    "        elif i%2 == 0:\n",
    "            template_txt += data_text[start_i:sp]\n",
    "        \n",
    "        elif i%2 != 0:\n",
    "            template_txt += \"<{0}>\".format(label_name[sp])\n",
    "            start_i = sp  \n",
    "            \n",
    "    return template_txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ラベル評価 Leave one out\n",
    "\n",
    "ex_txt_file = open(ex_txt, 'w')  #書き込みモードでオープン\n",
    "output_result = []\n",
    "failure_result = []\n",
    "success_result = []\n",
    "wh_result = {\"WHERE\":[], \"WHEN\":[], \"WHO\":[], \"WHAT\":[], \"HOW\":[], \"WHY\":[],\"SERIF\":[]}\n",
    "\n",
    "# ファイル読み込み用\n",
    "c = CorpusReader(corpus_file)\n",
    "all_sents = c.iob_sents('all') # データの読み込み\n",
    "loo = LeaveOneOut() # LeaveOneOut呼び出し\n",
    "\n",
    "for train_index, test_index in loo.split(all_sents):\n",
    "    \n",
    "    # 1. データ読み込み\n",
    "    X_train = [sent2features(all_sents[i]) for i in train_index] # 学習データの特徴量\n",
    "    y_train = [sent2labels(all_sents[i]) for i in train_index] # 学習データのラベル\n",
    "\n",
    "    X_test = [sent2features(all_sents[i]) for i in test_index] # テストデータの特徴量\n",
    "    y_test = [sent2labels(all_sents[i]) for i in test_index] # テストデータのラベル\n",
    "            \n",
    "    # 2.学習\n",
    "    # CRFモデルの準備\n",
    "    crf = sklearn_crfsuite.CRF(\n",
    "        algorithm='lbfgs',\n",
    "        c1=0.1,\n",
    "        c2=0.1,\n",
    "        max_iterations=100,\n",
    "        all_possible_transitions=True\n",
    "        )\n",
    "\n",
    "    crf.fit(X_train, y_train) # モデルの学習\n",
    "    predicted_label = crf.predict(X_test) # モデルの予測\n",
    "    \n",
    "    # 4.ラベル予測結果\n",
    "    # 4-1. データ用意\n",
    "    test_id = test_index[0]\n",
    "    example_sent = all_sents[test_id] \n",
    "    file_id = data_id_list[test_id]\n",
    "    text_data = sent2tokens(example_sent) # テキスト\n",
    "    text_line = \"\".join(text_data)\n",
    "    \n",
    "    p_data  = predicted_label[0] # テストデータの予測結果\n",
    "    c_data = y_test[0] # テストデータの正解\n",
    "    \n",
    "    true_wh_dic = label_report(text_data,c_data) # 5w1hラベルごとに単語を保存する\n",
    "    pred_wh_dic = label_report(text_data,p_data)  # 5w1hラベルごとに単語を保存する   \n",
    "\n",
    "    result = eval_report(true_wh_dic,pred_wh_dic) # 結果を4パターンで出力\n",
    "    wh_label = [\"WHERE\",\"WHO\",\"WHEN\",\"WHAT\"] # ラベル定義\n",
    "    c_r = cal_report(file_id,result,wh_label) # 4パターンの結果をまとめる\n",
    "    output_result.append(c_r)\n",
    "    \n",
    "    # 結果表示\n",
    "    print(file_id)\n",
    "    ex_txt_file.writelines(\"==== {0} ====\\n\".format(file_id))\n",
    "    ex_txt_file.writelines(\"{0}\\n\".format(text_line))\n",
    "    ex_txt_file.writelines(\"\\n\")\n",
    "    ex_txt_file.writelines(\"{0}\\n\".format(pred_wh_dic))\n",
    "    ex_txt_file.writelines(\"{0}\\n\".format(w2template(pred_wh_dic,text_line)))\n",
    "    \n",
    "    ex_txt_file.writelines(\"-----------------------------\\n\")\n",
    "    for wh_l in wh_label:\n",
    "        ex_txt_file.writelines(\"==== {0} ====\\n\".format(wh_l))\n",
    "        ex_txt_file.writelines(\"正解：{0}\\n\".format([t for t in true_wh_dic[wh_l]]))\n",
    "        ex_txt_file.writelines(\"予想：{0}\\n\".format([t for t in pred_wh_dic[wh_l]]))\n",
    "        ex_txt_file.writelines(\"ラベル誤り：{0}\\n\".format(result[wh_l][\"ラベル誤り\"]))\n",
    "        ex_txt_file.writelines(\"抽出漏れ：{0}\\n\".format(result[wh_l][\"抽出漏れ\"]))\n",
    "        ex_txt_file.writelines(\"過度の抽出：{0}\\n\".format(result[wh_l][\"過度の抽出\"]))\n",
    "   \n",
    "    ex_txt_file.writelines(\"-----------------------------\\n\") \n",
    "    ex_txt_file.writelines(\"{0}\\n\".format([\"記事id\",\"完全\",\"一部一致\",\"ラベル誤り\",\"抽出漏れ\",\"過度の抽出\"]))\n",
    "    ex_txt_file.writelines(\"{0}\\n\".format(c_r))\n",
    "\n",
    "    # 1記事における5w1hラベルごとの結果\n",
    "    for k,v in cal_report_wh(file_id,result).items():\n",
    "        wh_result[k].append(v) \n",
    "        \n",
    "    # 失敗データ・成功データのリスト化\n",
    "    for wh_l in wh_label:\n",
    "        for rt in result[wh_l][\"完全\"]:\n",
    "            success_result.append([file_id,wh_l,\"完全\",rt])\n",
    "        for rt in result[wh_l][\"一部一致\"]:\n",
    "            success_result.append([file_id,wh_l,\"一部一致\",rt])\n",
    "        for rt in result[wh_l][\"ラベル誤り\"]:\n",
    "            failure_result.append([file_id,wh_l,\"ラベル誤り\",rt])\n",
    "        for rt in result[wh_l][\"抽出漏れ\"]:\n",
    "            failure_result.append([file_id,wh_l,\"抽出漏れ\",rt])\n",
    "        for rt in result[wh_l][\"過度の抽出\"]:\n",
    "            failure_result.append([file_id,wh_l,\"過度の抽出\",rt])      \n",
    "\n",
    "ex_txt_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ファイル書き込み\n",
    "\n",
    "# 評価結果\n",
    "with open(ex_file, 'w') as f:\n",
    "    writer = csv.writer(f, lineterminator='\\n') # 改行コード（\\n）を指定しておく\n",
    "    writer.writerow([\"記事id\",\"完全\",\"一部一致\",\"ラベル誤り\",\"抽出漏れ\",\"過度の抽出\"])\n",
    "    for o_data in output_result:\n",
    "        writer.writerow(o_data)\n",
    "\n",
    "# 失敗データ\n",
    "with open(f_file, 'w') as f:\n",
    "    writer = csv.writer(f, lineterminator='\\n') # 改行コード（\\n）を指定しておく\n",
    "    writer.writerow([\"記事id\",\"ラベル\",\"失敗タイプ\",\"単語\",\"原因\"])\n",
    "    for f_data in failure_result:\n",
    "        writer.writerow(f_data)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
